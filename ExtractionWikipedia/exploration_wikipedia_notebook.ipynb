{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "import wikitextparser as wtp\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from wiki_dump_reader import iterate\n",
    "from tqdm import tqdm\n",
    "\n",
    "from external_sources_module import *\n",
    "from Identification_couples_livres.extract_books_from_DB import *\n",
    "\n",
    "if os.path.isfile('../Data/all_books.json'):\n",
    "    with open('../Data/all_books.json', 'r') as input_file:\n",
    "        all_books = json.load(input_file, cls=BookJSONDecoder)\n",
    "else:\n",
    "    all_books = generate_all_books()\n",
    "\n",
    "if os.path.isfile('../Data/author_ls.json'):\n",
    "    with open('../Data/author_ls.json', 'r') as input_file:\n",
    "        author_ls = json.load(input_file, cls=BookJSONDecoder)\n",
    "else:\n",
    "    author_ls, raw_author_ls = generate_all_authors(all_books)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Wikipédia\n",
    "\n",
    "## Extraction des pages Wikipédia\n",
    "\n",
    "### Parcours du dump et présélection des pages\n",
    "On sélectionne parmis le dump wikipédia les pages qui concernent des livres ou des auteurs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 3799564/20200401 [16:15<1:10:12, 3893.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_books:  16749\n",
      "count_authors:  40625\n"
     ]
    }
   ],
   "source": [
    "book_to_keep = {}\n",
    "writer_to_keep = {}\n",
    "count_author = 0\n",
    "count_book = 0\n",
    "\n",
    "for title, text in tqdm(iterate('../Data/Wikipedia/frwiki-20200401-pages-articles-multistream.xml'), total=20200401):\n",
    "\n",
    "    is_book_re = re.search(r\"Infobox (Ouvrage|Livre)\", text)\n",
    "    is_ebauche_re = re.search(r\"{{Ébauche\\|livre}}\", text)\n",
    "    if is_book_re or is_ebauche_re:\n",
    "        book_to_keep[title] = text\n",
    "        count_book += 1\n",
    "\n",
    "    is_writer_re = re.search(r\"Infobox Écrivain\", text)\n",
    "    is_Ecrivain_Quebecois_re = re.search(r\"Catégorie:Écrivain\", text)\n",
    "    if is_writer_re or is_Ecrivain_Quebecois_re:\n",
    "        writer_to_keep[title] = text\n",
    "        count_author += 1\n",
    "\n",
    "print('Nombre de page de livres extraits: ', count_book)\n",
    "with open('../Data/Wikipedia/fr_dumps_wikipedia_books.json', 'w') as outfile:\n",
    "    json.dump(book_to_keep, outfile)\n",
    "print('Nombre de page d\\'auteurs extraits: ', count_author)\n",
    "with open('../Data/Wikipedia/fr_dumps_wikipedia_writers.json', 'w') as outfile:\n",
    "    json.dump(writer_to_keep, outfile)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comparaison avec la base de donnée\n",
    "On recherche maintenant les auteurs et livres en communs avec la base de donnée"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16749/16749 [36:43<00:00,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre d'autheurs en commun:  3853  soit  23.004358469162337 %\n",
      "nombre d'autheurs reconnus:  13116  soit  3.4041007007526605  livre dans nos bases de donnée par livre reconnu dans wikipédia\n",
      "Frequence de reconnaissance:  [('Maria Chapdelaine', 92), ('Le Petit Chaperon rouge', 38), ('Les Anciens Canadiens', 38), (\"Les Chevaliers d'Émeraude\", 35), ('Poems (Agatha Christie)', 35), ('Un homme et son péché (roman)', 34), ('La Reine des anges', 31), ('La Reine des neiges (roman)', 30), ('La Reine des neiges', 30), ('Agaguk', 28)]\n",
      "repartition dans les bases de données: \n",
      "1/8:  {\"Depot_legal\": 2852, \"ADP\": 313, \"ILE\": 1396, \"Babelio\": 106}\n",
      "1/6:  {\"Depot_legal\": 700, \"ILE\": 320, \"ADP\": 83, \"Babelio\": 14}\n",
      "1/4:  {\"Depot_legal\": 4661, \"ILE\": 2028, \"Babelio\": 145, \"ADP\": 498}\n"
     ]
    }
   ],
   "source": [
    "# Couple dont on estime que les deux auteurs sont les mêmes, en fonction du critère (ex: 4 => 1/4)\n",
    "couple_books_4 = []\n",
    "couple_books_6 = []\n",
    "couple_books_8 = []\n",
    "# Compte les livres des auteurs par base de donnée (permet de voir si une base de donnée n'a aucun match)\n",
    "data_base_stats_4 = {}\n",
    "data_base_stats_6 = {}\n",
    "data_base_stats_8 = {}\n",
    "\n",
    "# Compte les auteurs de wikipédia qui ont été trouvé dans nos bases de donnée\n",
    "found_at_least_once = 0\n",
    "# Compte le nombre de couples\n",
    "nb_couple = 0\n",
    "# Compte le nombre de fois qu'un auteur de wikidia été trouvé dans nos bases de donnée\n",
    "author_found_freq = {}\n",
    "\n",
    "with open('../Data/Wikipedia/fr_dumps_wikipedia_books.json', 'r') as input_file:\n",
    "    wiki_books = json.load(input_file)\n",
    "\n",
    "for wiki_book, wiki_text in tqdm(wiki_books.items()):\n",
    "    # Compte le nombre de fois que l'auteur wikipédia a été associé a un auteur de nos bases de donnée\n",
    "    wiki_author_found_count = 0\n",
    "    normalized_wiki_book = normalize(wiki_book)\n",
    "    for DB_book in all_books:\n",
    "        dist_auteur = Levenshtein.distance(DB_book.title, normalized_wiki_book)\n",
    "        if dist_auteur < max(1, min(len(DB_book.title), len(normalized_wiki_book)) / 4):\n",
    "            new_couple = {\n",
    "                        'book DB': DB_book,\n",
    "                        'book wiki': {'title': normalized_wiki_book, 'title_raw':wiki_book, 'text': wiki_text}\n",
    "                    }\n",
    "            nb_couple += 1\n",
    "            wiki_author_found_count += 1\n",
    "\n",
    "            if dist_auteur < max(1, min(len(DB_book.title), len(wiki_book)) / 8):\n",
    "                    couple_books_8.append(new_couple)\n",
    "                    try:\n",
    "                        data_base_stats_8[DB_book.data_base] += 1\n",
    "                    except:\n",
    "                        data_base_stats_8[DB_book.data_base] = 1\n",
    "            elif dist_auteur < max(1, min(len(DB_book.title), len(wiki_book)) / 6):\n",
    "                couple_books_6.append(new_couple)\n",
    "                try:\n",
    "                    data_base_stats_6[DB_book.data_base] += 1\n",
    "                except:\n",
    "                    data_base_stats_6[DB_book.data_base] = 1\n",
    "            else:\n",
    "                couple_books_4.append(new_couple)\n",
    "                try:\n",
    "                    data_base_stats_4[DB_book.data_base] += 1\n",
    "                except:\n",
    "                    data_base_stats_4[DB_book.data_base] = 1\n",
    "\n",
    "    if wiki_author_found_count:\n",
    "        found_at_least_once += 1\n",
    "        author_found_freq[wiki_book] = wiki_author_found_count\n",
    "\n",
    "print(\"Nombre d'autheurs en commun: \", found_at_least_once, \" soit \", found_at_least_once * 100 / len(wiki_books), \"%\")\n",
    "print(\"Nombre d'autheurs reconnus: \", nb_couple, \" soit \", nb_couple/found_at_least_once, ' livre dans nos bases de donnée par livre reconnu dans wikipédia')\n",
    "print(\"\\nFrequence de reconnaissance des 10 auteurs de wikipédia les plus reconnus: \", sorted(author_found_freq.items(), key=lambda item: item[1], reverse=True)[:10])\n",
    "\n",
    "print(\"\\nRépartition des livres des auteurs reconnus dans les bases de données: \")\n",
    "print('1/8: ', json.dumps(data_base_stats_8))\n",
    "print('1/6: ', json.dumps(data_base_stats_6))\n",
    "print('1/4: ', json.dumps(data_base_stats_4))\n",
    "\n",
    "with open('../Data/Wikipedia/couple_books_wikipedia_4.json', 'w') as outfile:\n",
    "    json.dump(couple_books_4, outfile, cls=BookJSONEncoder)\n",
    "\n",
    "with open('../Data/Wikipedia/couple_books_wikipedia_6.json', 'w') as outfile:\n",
    "    json.dump(couple_books_6, outfile, cls=BookJSONEncoder)\n",
    "\n",
    "with open('../Data/Wikipedia/couple_books_wikipedia_8.json', 'w') as outfile:\n",
    "    json.dump(couple_books_8, outfile, cls=BookJSONEncoder)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Couple dont on estime que les deux auteurs sont les mêmes, en fonction du critère (ex: 4 => 1/4)\n",
    "couple_writers_4 = []\n",
    "couple_writers_6 = []\n",
    "couple_writers_8 = []\n",
    "\n",
    "# Compte les livres des auteurs par base de donnée (permet de voir si une base de donnée n'a aucun match)\n",
    "data_base_stats_4 = {}\n",
    "data_base_stats_6 = {}\n",
    "data_base_stats_8 = {}\n",
    "\n",
    "# Compte les auteurs de wikipédia qui ont été trouvé dans nos bases de donnée\n",
    "found_at_least_once = 0\n",
    "# Compte le nombre de couples\n",
    "nb_couple = 0\n",
    "# Compte le nombre de fois qu'un auteur de wikidia été trouvé dans nos bases de donnée\n",
    "author_found_freq = {}\n",
    "\n",
    "with open('../Wikipedia/fr_dumps_wikipedia_writers.json', 'r') as input_file:\n",
    "    wiki_authors = json.load(input_file)\n",
    "\n",
    "for wiki_author, wiki_author_text in tqdm(wiki_authors.items()):\n",
    "    # Compte le nombre de fois que l'auteur wikipédia a été associé a un auteur de nos bases de donnée\n",
    "    wiki_author_found_count = 0\n",
    "    normalized_wiki_author = normalize_author(wiki_author)\n",
    "    for DB_author, DB_author_books in author_ls.items():\n",
    "        dist_auteur = Levenshtein.distance(DB_author, normalized_wiki_author)\n",
    "\n",
    "        if dist_auteur < max(1, min(len(DB_author), len(normalized_wiki_author)) / 4):\n",
    "            new_couple = {\n",
    "                        'author_DB': {'name': DB_author, 'books': DB_author_books},\n",
    "                        'author_wiki': {'name': normalized_wiki_author, 'name_raw': wiki_author, 'text':wiki_author_text}\n",
    "                    }\n",
    "\n",
    "            nb_couple += 1\n",
    "            wiki_author_found_count += 1\n",
    "\n",
    "            if dist_auteur < max(1, min(len(DB_author), len(normalized_wiki_author)) / 8):\n",
    "                    couple_writers_8.append(new_couple)\n",
    "                    for book in DB_author_books:\n",
    "                        try:\n",
    "                            data_base_stats_8[book.data_base] += 1\n",
    "                        except:\n",
    "                            data_base_stats_8[book.data_base] = 1\n",
    "            elif dist_auteur < max(1, min(len(DB_author), len(normalized_wiki_author)) / 6):\n",
    "                couple_writers_6.append(new_couple)\n",
    "                for book in DB_author_books:\n",
    "                    try:\n",
    "                        data_base_stats_6[book.data_base] += 1\n",
    "                    except:\n",
    "                        data_base_stats_6[book.data_base] = 1\n",
    "            else:\n",
    "                couple_writers_4.append(new_couple)\n",
    "                for book in DB_author_books:\n",
    "                    try:\n",
    "                        data_base_stats_4[book.data_base] += 1\n",
    "                    except:\n",
    "                        data_base_stats_4[book.data_base] = 1\n",
    "\n",
    "    if wiki_author_found_count:\n",
    "        found_at_least_once += 1\n",
    "        author_found_freq[wiki_author] = wiki_author_found_count\n",
    "    # if count_freq_i > 10:\n",
    "    #     print(\"compte superieur a 10: \", author_DB)\n",
    "\n",
    "print(\"Nombre d'autheurs en commun: \", found_at_least_once, \" soit \", found_at_least_once * 100 / len(wiki_authors), \"%\")\n",
    "print(\"Nombre d'autheurs reconnus: \", nb_couple, \" soit \", nb_couple/found_at_least_once, ' auteur dans nos bases de donnée par auteur reconnu dans wikipedia')\n",
    "print(\"\\nFrequence de reconnaissance des 10 auteurs de wikipédia les plus reconnus: \", sorted(author_found_freq.items(), key=lambda item: item[1], reverse=True)[:10])\n",
    "\n",
    "\n",
    "print(\"\\nRépartition des livres des auteurs reconnus dans les bases de données: \")\n",
    "print('1/8: ', json.dumps(data_base_stats_8))\n",
    "print('1/6: ', json.dumps(data_base_stats_6))\n",
    "print('1/4: ', json.dumps(data_base_stats_4))\n",
    "\n",
    "with open('../Data/Wikipedia/couple_authors_wikipedia_4.json', 'w') as outfile:\n",
    "    json.dump(couple_writers_4, outfile, cls=BookJSONEncoder)\n",
    "\n",
    "with open('../Data/Wikipedia/couple_authors_wikipedia_6.json', 'w') as outfile:\n",
    "    json.dump(couple_writers_6, outfile, cls=BookJSONEncoder)\n",
    "\n",
    "with open('../Data/Wikipedia/couple_authors_wikipedia_8.json', 'w') as outfile:\n",
    "    json.dump(couple_writers_8, outfile, cls=BookJSONEncoder)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parsing des informations de la page wikipédia des couples extraits"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2412/2412 [00:10<00:00, 231.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "titre des sections:  [\n",
      "  [\n",
      "    null,\n",
      "    2412\n",
      "  ],\n",
      "  [\n",
      "    \" Liens externes \",\n",
      "    1977\n",
      "  ],\n",
      "  [\n",
      "    \" Biographie \",\n",
      "    1720\n",
      "  ],\n",
      "  [\n",
      "    \" Notes et r\\u00e9f\\u00e9rences \",\n",
      "    1423\n",
      "  ],\n",
      "  [\n",
      "    \" \\u0152uvres \",\n",
      "    810\n",
      "  ],\n",
      "  [\n",
      "    \" Bibliographie \",\n",
      "    706\n",
      "  ],\n",
      "  [\n",
      "    \" Romans \",\n",
      "    521\n",
      "  ],\n",
      "  [\n",
      "    \" \\u0152uvre \",\n",
      "    458\n",
      "  ],\n",
      "  [\n",
      "    \" Voir aussi \",\n",
      "    417\n",
      "  ],\n",
      "  [\n",
      "    \" R\\u00e9f\\u00e9rences \",\n",
      "    409\n",
      "  ],\n",
      "  [\n",
      "    \" Honneurs \",\n",
      "    298\n",
      "  ],\n",
      "  [\n",
      "    \" Articles connexes \",\n",
      "    291\n",
      "  ],\n",
      "  [\n",
      "    \" Publications \",\n",
      "    223\n",
      "  ],\n",
      "  [\n",
      "    \" Th\\u00e9\\u00e2tre \",\n",
      "    211\n",
      "  ],\n",
      "  [\n",
      "    \" Annexes \",\n",
      "    208\n",
      "  ],\n",
      "  [\n",
      "    \" Po\\u00e9sie \",\n",
      "    189\n",
      "  ],\n",
      "  [\n",
      "    \" Nouvelles \",\n",
      "    179\n",
      "  ],\n",
      "  [\n",
      "    \" Prix et distinctions \",\n",
      "    176\n",
      "  ],\n",
      "  [\n",
      "    \" Filmographie \",\n",
      "    165\n",
      "  ],\n",
      "  [\n",
      "    \" Essais \",\n",
      "    152\n",
      "  ],\n",
      "  [\n",
      "    \" Sources \",\n",
      "    147\n",
      "  ],\n",
      "  [\n",
      "    \" Distinctions \",\n",
      "    141\n",
      "  ],\n",
      "  [\n",
      "    \" Notes \",\n",
      "    124\n",
      "  ],\n",
      "  [\n",
      "    \"Biographie\",\n",
      "    116\n",
      "  ],\n",
      "  [\n",
      "    \" Recueils de nouvelles \",\n",
      "    106\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "with open('../Data/Wikipedia/couple_authors_wikipedia_8.json', 'r') as outfile:\n",
    "    couple_writers = json.load(outfile, cls=BookJSONDecoder)\n",
    "\n",
    "# On compte les intitulés des sections pour la suite\n",
    "title_dict = {}\n",
    "\n",
    "for couple in tqdm(couple_writers):\n",
    "\n",
    "    info_box, page = split_info_box(couple['author_wiki']['text'])\n",
    "\n",
    "    couple['author_wiki']['info_box'] = get_info_from_infobox(info_box)\n",
    "\n",
    "    parsed_page = wtp.parse(format_list(page))\n",
    "    sections_infos = {}\n",
    "    section_titles = set()\n",
    "\n",
    "    for section in parsed_page.sections:\n",
    "        if section.level <= 2:\n",
    "            section_titles.add(section.title)\n",
    "            sections_infos[section.title] = parse_section(section, section_titles, section.level)\n",
    "\n",
    "\n",
    "    couple['author_wiki']['parsed_text'] = sections_infos\n",
    "\n",
    "    # On met a jour le compte des intitulés des sections\n",
    "    for title in section_titles:\n",
    "        try:\n",
    "            title_dict[title] += 1\n",
    "        except KeyError:\n",
    "            title_dict[title] = 1\n",
    "\n",
    "\n",
    "print(\"Titre des sections les plus fréquentes: \")\n",
    "for name, nb in dict(sorted(title_dict.items(), key=lambda item: item[1], reverse=True)[:15]).items():\n",
    "    print('Section: ', name + \" \" * (30 - len(name)),  ' compte: ', nb)\n",
    "\n",
    "with open('../Data/Wikipedia/parsed_couple_writers.json', 'w') as outfile:\n",
    "    json.dump(couple_writers, outfile, cls=BookJSONEncoder)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Confirmation des les couples par leurs informations\n",
    "On s'intéresse aux information récoltées sur ces auteurs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('../Data/Wikipedia/parsed_couple_writers.json', 'r') as input_file:\n",
    "    couples_writers = json.load(input_file, cls=BookJSONDecoder)\n",
    "\n",
    "count_awards = 0\n",
    "nb_awards_for_each_wiki_author = []\n",
    "\n",
    "all_awards_name = set()\n",
    "\n",
    "for couple in couples_writers:\n",
    "    award_titles = set()\n",
    "\n",
    "    for title, section in couple['author_wiki']['parsed_text'].items():\n",
    "        if re.search(r'prix|distinction', title, re.IGNORECASE):\n",
    "            awards = extract_title_from_list(section['list'])\n",
    "            award_titles.update(awards)\n",
    "\n",
    "    if award_titles:\n",
    "        count_awards += 1\n",
    "        all_awards_name.update(award_titles)\n",
    "    nb_awards_for_each_wiki_author.append(len(award_titles))\n",
    "print(\"Nombre d'auteur dont on a récupéré des prix: \", count_awards)\n",
    "print(\"Nombre total de prix: \", sum(nb_awards_for_each_wiki_author))\n",
    "\n",
    "print(\"exemple des prix: \", all_awards_name[:20])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "On veut confirmer nos couples d'auteur a partir des titres des livres qu'ils ont ecrits\n",
    "On va comparer les titres uns à uns.\n",
    "- Si un des titre ou plus correspond, alors on confirmera ce couple,\n",
    "- Si aucune titre n'est en commun, on considera le couple comme faux ou incomplet\n",
    "- Si on n'a pas récupéré de titre depuis wikidata alors on ne peut pas conclure"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('../Data/Wikipedia/parsed_couple_writers.json', 'r') as couples_file:\n",
    "    couple_writers = json.load(couples_file, cls=BookJSONDecoder)\n",
    "\n",
    "# Listes pour stoker nos couples confirmé, invalidé et ceux dont on ne peut pas conclure car il n'y a pas de titres\n",
    "couples_confirmed = []\n",
    "couples_infirmed = []\n",
    "couples_unsure = []\n",
    "# Compte les cas ci-dessus\n",
    "count_checked_by_titles = 0\n",
    "count_differentiate_by_titles = 0\n",
    "count_no_titles = 0\n",
    "\n",
    "for couple in tqdm(couple_writers):\n",
    "    confirmed_by_title_match = False\n",
    "    confirmed_by_title_match_re = False\n",
    "    book_titles = set()\n",
    "    book_titles_raw = set()\n",
    "\n",
    "    # On recupère tout les titres des livres de la page sous forme de set\n",
    "    for title, section in couple['author_wiki']['parsed_text'].items():\n",
    "        try:\n",
    "            if re.search(r'bibliographie|Œuvres|Œuvre|théâtre|poésie|nouvelles|essais|publications|romans|roman|Littérature|Ouvrages|Filmographie', title, re.IGNORECASE):\n",
    "                book_titles.update(extract_title_from_list(section['list']))\n",
    "                book_titles_raw.update(section['list'])\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    # On vérifie si on peut trouver des titres proches en comparant les titres parsés\n",
    "    # au titres normalisés de nos bases de donnée\n",
    "    book_titles_DB = [book.title for book in couple['author_DB']['books']]\n",
    "    for title in book_titles_DB:\n",
    "        for title_wiki in book_titles:\n",
    "            dist_titles = Levenshtein.distance(title, title_wiki)\n",
    "            if dist_titles < max(1, min(len(title), len(title_wiki)) / 4):\n",
    "                confirmed_by_title_match = True\n",
    "                break\n",
    "    # On recherche par le suite le titre du livre directement dans les titres non nettoyé de la page wikipédia\n",
    "    # via expression régulières\n",
    "    for title in book_titles_DB:\n",
    "        for title_wiki_raw in book_titles_raw:\n",
    "            if re.search(normalize(title), normalize(title_wiki_raw)):\n",
    "                confirmed_by_title_match_re = True\n",
    "                break\n",
    "\n",
    "    # On liste les cas possibles: 0,1,2 tests réussis\n",
    "    if (not confirmed_by_title_match) and (not confirmed_by_title_match_re):\n",
    "        if book_titles and book_titles_DB:\n",
    "            count_differentiate_by_titles += 1\n",
    "            couples_infirmed.append(couple)\n",
    "        else:\n",
    "            count_no_titles += 1\n",
    "            couples_unsure.append(couple)\n",
    "    else:\n",
    "        count_checked_by_titles += 1\n",
    "        couples_confirmed.append(couple)\n",
    "\n",
    "    # On sauvegarde comment avons nous trouvé les correspondances entre les deux listes de titres\n",
    "    couple['confirmed_by_title_match'] = confirmed_by_title_match\n",
    "    couple['confirmed_by_title_match_re'] = confirmed_by_title_match_re\n",
    "\n",
    "print('Nombre de couple: ', len(couple_writers))\n",
    "print('Nombre de couples confirmés par titres: ', count_checked_by_titles)\n",
    "print('Nombre de couples infirmés par titres: ', count_differentiate_by_titles)\n",
    "print('Nombre de couples sans titres de wikipedia: ', count_no_titles)\n",
    "\n",
    "with open('../Data/Wikipedia/couple_authors_wikipedia_confirmed.json', 'w') as outfile:\n",
    "    json.dump(couples_confirmed, outfile, cls=BookJSONEncoder)\n",
    "\n",
    "with open('../Data/Wikipedia/couple_authors_wikipedia_infirmed.json', 'w') as outfile:\n",
    "    json.dump(couples_infirmed, outfile, cls=BookJSONEncoder)\n",
    "\n",
    "with open('../Data/Wikipedia/couple_authors_wikipedia_unsure.json', 'w') as outfile:\n",
    "    json.dump(couples_unsure, outfile, cls=BookJSONEncoder)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "On peut s'interésé a l'intersection des couples généré depuis wikidata"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'auteurs en communs:  765\n"
     ]
    }
   ],
   "source": [
    "with open('../Data/Wikipedia/parsed_couple_writers.json', 'r') as couples_file:\n",
    "    wikipedia_couple_writers = json.load(couples_file)\n",
    "\n",
    "with open('../Data/Wikidata/couple_authors_wikidata.json', 'r') as couples_file:\n",
    "    wikidata_couple_writers = json.load(couples_file)\n",
    "\n",
    "count = 0\n",
    "couples_wikipedia_wikidata = []\n",
    "for wikipedia_couple in wikipedia_couple_writers:\n",
    "    for wikidata_couple in wikidata_couple_writers:\n",
    "        if wikidata_couple['author_DB']['name'] == wikipedia_couple['author_DB']['name']:\n",
    "            count += 1\n",
    "print(\"Nombre d'auteurs en communs: \", count)\n",
    "\n",
    "with open('../Data/couple_authors_wikipedia_wikidata.json', 'w') as outfile:\n",
    "    json.dump(couples_wikipedia_wikidata, outfile)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}