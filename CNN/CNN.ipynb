{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras_metrics in c:\\users\\ares\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: owlready2 in c:\\users\\ares\\anaconda3\\lib\\site-packages (0.24)\n",
      "Requirement already satisfied: pprint in c:\\users\\ares\\anaconda3\\lib\\site-packages (0.1)\n",
      "Requirement already satisfied: Keras>=2.1.5 in c:\\users\\ares\\appdata\\roaming\\python\\python37\\site-packages (from keras_metrics) (2.3.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\ares\\appdata\\roaming\\python\\python37\\site-packages (from Keras>=2.1.5->keras_metrics) (1.1.2)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\ares\\anaconda3\\lib\\site-packages (from Keras>=2.1.5->keras_metrics) (1.14.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\ares\\appdata\\roaming\\python\\python37\\site-packages (from Keras>=2.1.5->keras_metrics) (1.0.8)\n",
      "Requirement already satisfied: h5py in c:\\users\\ares\\anaconda3\\lib\\site-packages (from Keras>=2.1.5->keras_metrics) (2.10.0)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\ares\\anaconda3\\lib\\site-packages (from Keras>=2.1.5->keras_metrics) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\ares\\anaconda3\\lib\\site-packages (from Keras>=2.1.5->keras_metrics) (1.18.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\ares\\anaconda3\\lib\\site-packages (from Keras>=2.1.5->keras_metrics) (5.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras_metrics owlready2 pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/training.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-54f65a24740e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m         \u001b[0mtraining_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/content/training.npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m         \u001b[0mvalid_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/content/validation.npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/training.npy'"
     ]
    }
   ],
   "source": [
    "from dataset_utils_2 import compute_max_length, preprocess_dataset, get_classes_weights\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "from time import time\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, GaussianNoise\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, Embedding\n",
    "from keras_metrics import precision, recall, f1_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.keras import balanced_batch_generator\n",
    "from nltk.metrics import edit_distance\n",
    "from keras.utils import plot_model\n",
    "\n",
    "def create_model(input_length, embedding):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(128, 300, input_length=input_length, weights=[embedding]))\n",
    "    model.add(Conv1D(64, 5, padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D(64, 5))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    \n",
    "    model.add(Conv1D(64, 5, padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D(64, 5))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    \n",
    "    model.add(Conv1D(64, 5, padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D(64, 5))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    \n",
    "    model.add(Conv1D(64, 5, padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D(64, 5))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    \n",
    "    model.add(Conv1D(64, 5, padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D(64, 5))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    \n",
    "    model.add(Conv1D(64, 5, padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D(64, 5))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    \n",
    "    model.add(Conv1D(64, 5, padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D(64, 5))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    \n",
    "    model.add(Conv1D(64, 5, padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D(64, 5))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(500))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(500))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(500))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(500))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(500))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(500))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(500))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(500))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(500))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    opt = keras.optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, decay=0.0, amsgrad=False)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=[binary_precision(), binary_recall(), binary_f1_score()])\n",
    "    return model\n",
    "\n",
    "def prepare_embedding(filepath):\n",
    "    embedding_vectors = {}\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            line_split = line.strip().split(\" \")\n",
    "            vec = np.array(line_split[1:], dtype=float)\n",
    "            char = line_split[0]\n",
    "            embedding_vectors[char] = vec\n",
    "\n",
    "    embedding_matrix = np.zeros((128, 300))\n",
    "    for i in range(128):\n",
    "        embedding_vector = embedding_vectors.get(chr(i))\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "def train(model, train_data, valid_data):\n",
    "    nb_examples = train_data.shape[0]\n",
    "    x_train = train_data[:nb_examples, :-1]\n",
    "    y_train = train_data[:nb_examples, -1]\n",
    "    \n",
    "#     x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "\n",
    "    x_valid = valid_data[:, :-1]\n",
    "    y_valid = valid_data[:, -1]\n",
    "    \n",
    "#     x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1], 1)\n",
    "\n",
    "#     y_train = keras.utils.to_categorical(y_train, 2)\n",
    "#     y_valid = keras.utils.to_categorical(y_valid, 2)\n",
    "\n",
    "#     training_generator, steps_per_epoch = balanced_batch_generator(x_train, y_train, sampler=RandomOverSampler(), batch_size=32, random_state=42)\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=1, mode='min')\n",
    "    cp = ModelCheckpoint(\"model.h5\", save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "#     model.fit_generator(generator=training_generator,\n",
    "#                         steps_per_epoch=steps_per_epoch,\n",
    "#                         epochs=50,\n",
    "#                         validation_data=(x_valid, y_valid),\n",
    "#                         shuffle=True,\n",
    "#                         class_weight={0: 1, 1: 5},\n",
    "#                         callbacks=[es, cp])\n",
    "\n",
    "    neg_weight = 40\n",
    "\n",
    "    valid_weights = np.array([neg_weight if y == 0 else 1 for y in y_valid])\n",
    "\n",
    "    model.fit(x=x_train,\n",
    "              y=y_train,\n",
    "              epochs=100,\n",
    "              batch_size=64,\n",
    "              validation_data=(x_valid, y_valid, valid_weights),\n",
    "              shuffle=True,\n",
    "              class_weight={0: neg_weight, 1: 1},\n",
    "              callbacks=[es, cp])\n",
    "\n",
    "def test_model(model, test_datasets, distances=None):\n",
    "    for i, ds in enumerate(test_datasets):\n",
    "        x_test = ds[:, :-1]\n",
    "#         x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)\n",
    "        y_test = ds[:, -1]\n",
    "#         y_test = keras.utils.to_categorical(y_test, 2)\n",
    "#         scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "#         print('Test loss:', scores[0])\n",
    "#         print('Test accuracy:', scores[1])\n",
    "#         print('Test recall:', scores[2])\n",
    "#         print('Test f1-score:', scores[3])\n",
    "\n",
    "\n",
    "        preds = model.predict(x_test)\n",
    "        preds = [1 if preds[i][0] > 0.1 else 0 for i in range(preds.shape[0])]\n",
    "        tp_ = tp(preds, y_test)\n",
    "        fp_ = fp(preds, y_test)\n",
    "        fn_ = fn(preds, y_test)\n",
    "        p = precision(tp_, fp_)\n",
    "        r = recall(tp_, fn_)\n",
    "        f1 = f1_score(p, r)\n",
    "\n",
    "        print(f\"tp: {tp_}, nb_pos: {len([1 for y in y_test if y == 1])}\")\n",
    "        print(f\"fp: {fp_}, nb_pos: {len([1 for y in y_test if y == 1])}\")\n",
    "        print(f\"fn: {fn_}, nb_neg: {len([1 for y in y_test if y == 0])}\")\n",
    "        print(f\"precision: {p}, recall: {r}, f1: {f1}\")\n",
    "    \n",
    "        if distances:\n",
    "            thresh = 1\n",
    "            preds = [preds[j] if distances[i][j] < thresh else 0 for j in range(len(preds))]\n",
    "            tp_ = tp(preds, y_test)\n",
    "            fp_ = fp(preds, y_test)\n",
    "            fn_ = fn(preds, y_test)\n",
    "            p = precision(tp_, fp_)\n",
    "            r = recall(tp_, fn_)\n",
    "            f1 = f1_score(p, r)\n",
    "\n",
    "            print(f\"tp: {tp_}, nb_pos: {len([1 for y in y_test if y == 1])}\")\n",
    "            print(f\"fp: {fp_}, nb_pos: {len([1 for y in y_test if y == 1])}\")\n",
    "            print(f\"fn: {fn_}, nb_neg: {len([1 for y in y_test if y == 0])}\")\n",
    "            print(f\"better: precision: {p}, recall: {r}, f1: {f1}\")\n",
    "            \n",
    "def tp(preds, targets):\n",
    "    tp = len([1 for i in range(len(preds)) if preds[i] == 1 and targets[i] == 1])\n",
    "    \n",
    "    return tp\n",
    "\n",
    "def fp(preds, targets):\n",
    "    fp = len([1 for i in range(len(preds)) if preds[i] == 1 and targets[i] == 0])\n",
    "    \n",
    "    return fp\n",
    "\n",
    "def tn(preds, targets):\n",
    "    tn = len([1 for i in range(len(preds)) if preds[i] == 0 and targets[i] == 0])\n",
    "\n",
    "def fn(preds, targets):\n",
    "    fn = len([1 for i in range(len(preds)) if preds[i] == 0 and targets[i] == 1])\n",
    "    \n",
    "    return fn\n",
    "    \n",
    "def precision(tp_, fp_):    \n",
    "    return tp_ / (tp_ + fp_)\n",
    "\n",
    "def recall(tp_, fn_):\n",
    "    return tp_ / (tp_ + fn_)\n",
    "\n",
    "def f1_score(p, r):\n",
    "    return 2 * p * r / (p + r)\n",
    "\n",
    "def test_batch(models, test_datasets):\n",
    "    best_model = None\n",
    "    best_error = float(\"inf\")\n",
    "    for m in models:\n",
    "        error = 0\n",
    "    for ds in test_datasets:\n",
    "        x_test = ds[:, :-1]\n",
    "    #       x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)\n",
    "        y_test = ds[:, -1]\n",
    "    #       y_test = keras.utils.to_categorical(y_test, 2)\n",
    "        scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "        loss = scores[0]\n",
    "        error += loss\n",
    "\n",
    "    error = error / len(test_datasets)\n",
    "    if error < best_error:\n",
    "        best_error = error\n",
    "        best_model = model\n",
    "\n",
    "    best_model.save(\"best_model.h5\")\n",
    "    test_model(best_model, test_datasets)\n",
    "    \n",
    "def explore_results(model, dataset, text):\n",
    "    x_test = dataset[:, :-1]\n",
    "    y_test = dataset[:, -1]\n",
    "    \n",
    "    preds = model.predict_classes(x_test)\n",
    "    \n",
    "    for i, y in enumerate(y_test):\n",
    "        if y == 0 and y != preds[i]:\n",
    "            print(text.iloc[i, :])\n",
    "    \n",
    "def test_dataset(model, dataset, max_length, nb_subclasses, nb_superclasses):\n",
    "    \n",
    "    test_df = pd.DataFrame()\n",
    "    \n",
    "    for _, row in dataset.iterrows():\n",
    "        if row[\"class_1\"] != row[\"class_2\"]:\n",
    "            test_df = test_df.append(row, ignore_index=True)\n",
    "            \n",
    "    test_df.to_csv(\"temp.csv\", index=False)\n",
    "            \n",
    "    _, _, test_dataset = preprocess_dataset(None, None, [\"temp.csv\"],\n",
    "                                            max_length, nb_subclasses, nb_superclasses)\n",
    "    \n",
    "    test_dataset = test_dataset[0]\n",
    "            \n",
    "    x_test = test_dataset[:, :-1]\n",
    "    y_test = test_dataset[:, -1]\n",
    "    \n",
    "    preds = model.predict_classes(x_test)\n",
    "    \n",
    "    tp_ = len(dataset) - len(test_df) + tp(preds, y_test)\n",
    "    fn_ = fn(preds, y_test)\n",
    "    \n",
    "    print(tp_)\n",
    "    print(fn_)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # max_length = compute_max_length(dataset)\n",
    "    max_length = 150\n",
    "\n",
    "    nb_subclasses = 0\n",
    "    nb_superclasses = 5\n",
    "    \n",
    "    load = False\n",
    "    \n",
    "#     print(\"Loading training and validation dataset...\")\n",
    "\n",
    "    if load:\n",
    "        (training_dataset, valid_dataset, test_dataset) = preprocess_dataset(\n",
    "            \"Training Dataset.csv\",\n",
    "            \"Validation Dataset.csv\",\n",
    "            [\"Test Dataset.csv\"], max_length, nb_subclasses, nb_superclasses\n",
    "        )\n",
    "\n",
    "        np.save(\"training\", training_dataset)\n",
    "        np.save(\"validation\", valid_dataset)\n",
    "\n",
    "    else:\n",
    "        training_dataset = np.load(\"/content/training.npy\")\n",
    "        valid_dataset = np.load(\"/content/validation.npy\")\n",
    "\n",
    "    print(\"Loading test datasets...\")\n",
    "    \n",
    "    if load:\n",
    "        (_, _, test_dataset) = preprocess_dataset(None, None, [\n",
    "            \"Test Dataset.csv\"], max_length, nb_subclasses, nb_superclasses)\n",
    "\n",
    "        np.save(\"test\", test_dataset)\n",
    "\n",
    "    else:\n",
    "        test_dataset = np.load(\"test.npy\", allow_pickle=True)\n",
    "    \n",
    "    test_files = [\n",
    "        \"Dataset Mouse-Human2.csv\",\n",
    "        \"Training FMA-NCI 2.csv\",\n",
    "        \"Training FMA-SNOMED 2.csv\",\n",
    "        \"Training SNOMED-NCI 2.csv\",\n",
    "        \"Dataset envo-sweet.csv\",\n",
    "        \"Dataset flopo-pto.csv\"]\n",
    "       \n",
    "    distances = []\n",
    "    for f in test_files:\n",
    "        ds = pd.read_csv(f)\n",
    "        dist = []\n",
    "        for _, row in ds.iterrows():\n",
    "            c1 = row[\"class_1\"].lower().replace(\"_\", \" \")\n",
    "            c2 = row[\"class_2\"].lower().replace(\"_\", \" \")\n",
    "\n",
    "            d = edit_distance(c1, c2) / min(len(c1), len(c2))\n",
    "            dist.append(d)\n",
    "        distances.append(dist)\n",
    "\n",
    "    training_dataset = training_dataset[np.random.choice(training_dataset.shape[0], 40000, replace=False)]\n",
    "    embedding = prepare_embedding(\"glove.840B.300d-char.txt\")\n",
    "\n",
    "    model = create_model(2*(1 + nb_subclasses + nb_superclasses)*max_length, embedding)\n",
    "    train(model, training_dataset, valid_dataset)\n",
    "    # model.load_weights(\"best_model.h5\")\n",
    "    test_model(model, test_dataset)\n",
    "    explore_results(model, test_dataset[0], pd.read_csv(\"Test Dataset.csv\"))\n",
    "    model.save(\"model_test_full_2.h5\")\n",
    "    test_ds = pd.read_csv(\"Positive Dataset.csv\")\n",
    "    test_dataset(model, test_ds, max_length, nb_subclasses, nb_superclasses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrÃ©er dataset de training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
