{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "import wikitextparser as wtp\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from wiki_dump_reader import iterate\n",
    "from tqdm import tqdm\n",
    "\n",
    "from external_sources_module import *\n",
    "from Identification_couples_livres.extract_books_from_DB import *\n",
    "\n",
    "if os.path.isfile('../Data/all_books.json'):\n",
    "    with open('../Data/all_books.json', 'r') as input_file:\n",
    "        all_books = json.load(input_file, cls=BookJSONDecoder)\n",
    "else:\n",
    "    all_books = generate_all_books()\n",
    "\n",
    "if os.path.isfile('../Data/author_ls.json'):\n",
    "    with open('../Data/author_ls.json', 'r') as input_file:\n",
    "        author_ls = json.load(input_file, cls=BookJSONDecoder)\n",
    "else:\n",
    "    author_ls, raw_author_ls = generate_all_authors(all_books)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Wikipédia\n",
    "\n",
    "## Extraction des pages Wikipédia\n",
    "\n",
    "### Parcours du dump et présélection des pages\n",
    "On sélectionne parmis le dump wikipédia les pages qui concernent des livres ou des auteurs selon \n",
    "leur infobox ou leur catégorisation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 3799564/20200401 [18:20<1:19:09, 3452.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de page de livres extraits:  16749\n",
      "Nombre de page d'auteurs extraits:  40625\n"
     ]
    }
   ],
   "source": [
    "book_to_keep = {}\n",
    "writer_to_keep = {}\n",
    "count_author = 0\n",
    "count_book = 0\n",
    "\n",
    "for title, text in tqdm(iterate('../Data/Wikipedia/frwiki-20200401-pages-articles-multistream.xml'), total=3799564):\n",
    "\n",
    "    is_book_re = re.search(r\"Infobox (Ouvrage|Livre)\", text)\n",
    "    is_ebauche_re = re.search(r\"{{Ébauche\\|livre}}\", text)\n",
    "    if is_book_re or is_ebauche_re:\n",
    "        book_to_keep[title] = text\n",
    "        count_book += 1\n",
    "\n",
    "    is_writer_re = re.search(r\"Infobox Écrivain\", text)\n",
    "    is_Ecrivain_Quebecois_re = re.search(r\"Catégorie:Écrivain\", text)\n",
    "    if is_writer_re or is_Ecrivain_Quebecois_re:\n",
    "        writer_to_keep[title] = text\n",
    "        count_author += 1\n",
    "\n",
    "print('Nombre de page de livres extraits: ', count_book)\n",
    "with open('../Data/Wikipedia/fr_dumps_wikipedia_books.json', 'w') as outfile:\n",
    "    json.dump(book_to_keep, outfile)\n",
    "print('Nombre de page d\\'auteurs extraits: ', count_author)\n",
    "with open('../Data/Wikipedia/fr_dumps_wikipedia_writers.json', 'w') as outfile:\n",
    "    json.dump(writer_to_keep, outfile)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comparaison avec la base de donnée\n",
    "On recherche maintenant les auteurs et livres en communs avec la base de donnée"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16749/16749 [37:51<00:00,  7.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'autheurs en commun:  3863  soit  23.064063526180668 %\n",
      "Nombre d'autheurs reconnus:  13534  soit  3.503494693243593  livre dans nos bases de donnée par livre reconnu dans wikipédia\n",
      "\n",
      "Frequence de reconnaissance des 10 auteurs de wikipédia les plus reconnus:  [('Maria Chapdelaine', 94), ('Poems (Agatha Christie)', 39), ('Le Petit Chaperon rouge', 38), ('Les Anciens Canadiens', 38), (\"Les Chevaliers d'Émeraude\", 35), ('Un homme et son péché (roman)', 34), ('La Reine des anges', 30), ('La Reine des neiges (roman)', 29), ('Agaguk', 29), ('La Reine des neiges', 29)]\n",
      "\n",
      "Répartition des livres des auteurs reconnus dans les bases de données: \n",
      "1/8:  {\"Depot_legal\": 2852, \"ADP\": 313, \"ILE\": 1396, \"Babelio\": 256}\n",
      "1/6:  {\"Depot_legal\": 700, \"ILE\": 320, \"ADP\": 83, \"Babelio\": 59}\n",
      "1/4:  {\"Depot_legal\": 4661, \"ILE\": 2028, \"Babelio\": 368, \"ADP\": 498}\n"
     ]
    }
   ],
   "source": [
    "# Couple dont on estime que les deux auteurs sont les mêmes, en fonction du critère (ex: 4 => 1/4)\n",
    "couple_books_4 = []\n",
    "couple_books_6 = []\n",
    "couple_books_8 = []\n",
    "# Compte les livres des auteurs par base de donnée (permet de voir si une base de donnée n'a aucun match)\n",
    "data_base_stats_4 = {}\n",
    "data_base_stats_6 = {}\n",
    "data_base_stats_8 = {}\n",
    "\n",
    "# Compte les auteurs de wikipédia qui ont été trouvé dans nos bases de donnée\n",
    "found_at_least_once = 0\n",
    "# Compte le nombre de couples\n",
    "nb_couple = 0\n",
    "# Compte le nombre de fois qu'un auteur de wikidia été trouvé dans nos bases de donnée\n",
    "author_found_freq = {}\n",
    "\n",
    "with open('../Data/Wikipedia/fr_dumps_wikipedia_books.json', 'r') as input_file:\n",
    "    wiki_books = json.load(input_file)\n",
    "\n",
    "for wiki_book, wiki_text in tqdm(wiki_books.items()):\n",
    "    # Compte le nombre de fois que l'auteur wikipédia a été associé a un auteur de nos bases de donnée\n",
    "    wiki_author_found_count = 0\n",
    "    normalized_wiki_book = normalize(wiki_book)\n",
    "    for DB_book in all_books:\n",
    "        dist_title = Levenshtein.distance(DB_book.title, normalized_wiki_book)\n",
    "        if dist_title < max(1, min(len(DB_book.title), len(normalized_wiki_book)) / 4):\n",
    "            new_couple = {\n",
    "                        'book DB': DB_book,\n",
    "                        'book wiki': {'title': normalized_wiki_book, 'title_raw':wiki_book, 'text': wiki_text}\n",
    "                    }\n",
    "            nb_couple += 1\n",
    "            wiki_author_found_count += 1\n",
    "\n",
    "            if dist_title < max(1, min(len(DB_book.title), len(wiki_book)) / 8):\n",
    "                    couple_books_8.append(new_couple)\n",
    "                    try:\n",
    "                        data_base_stats_8[DB_book.data_base] += 1\n",
    "                    except:\n",
    "                        data_base_stats_8[DB_book.data_base] = 1\n",
    "            elif dist_title < max(1, min(len(DB_book.title), len(wiki_book)) / 6):\n",
    "                couple_books_6.append(new_couple)\n",
    "                try:\n",
    "                    data_base_stats_6[DB_book.data_base] += 1\n",
    "                except:\n",
    "                    data_base_stats_6[DB_book.data_base] = 1\n",
    "            else:\n",
    "                couple_books_4.append(new_couple)\n",
    "                try:\n",
    "                    data_base_stats_4[DB_book.data_base] += 1\n",
    "                except:\n",
    "                    data_base_stats_4[DB_book.data_base] = 1\n",
    "\n",
    "    if wiki_author_found_count:\n",
    "        found_at_least_once += 1\n",
    "        author_found_freq[wiki_book] = wiki_author_found_count\n",
    "\n",
    "print(\"Nombre de livres en commun: \", found_at_least_once, \" soit \", found_at_least_once * 100 / len(wiki_books), \"%\")\n",
    "print(\"Nombre de livres reconnus: \", nb_couple, \" soit \", nb_couple/found_at_least_once, ' livre dans nos bases de donnée par livre reconnu dans wikipédia')\n",
    "print(\"\\nFréquence de reconnaissance des 10 livres de wikipédia les plus reconnus: \", sorted(author_found_freq.items(), key=lambda item: item[1], reverse=True)[:10])\n",
    "\n",
    "print(\"\\nRépartition des livres des auteurs reconnus dans les bases de données: \")\n",
    "print('1/8: ', json.dumps(data_base_stats_8))\n",
    "print('1/6: ', json.dumps(data_base_stats_6))\n",
    "print('1/4: ', json.dumps(data_base_stats_4))\n",
    "\n",
    "with open('../Data/Wikipedia/couple_books_wikipedia_4.json', 'w') as outfile:\n",
    "    json.dump(couple_books_4, outfile, cls=BookJSONEncoder)\n",
    "\n",
    "with open('../Data/Wikipedia/couple_books_wikipedia_6.json', 'w') as outfile:\n",
    "    json.dump(couple_books_6, outfile, cls=BookJSONEncoder)\n",
    "\n",
    "with open('../Data/Wikipedia/couple_books_wikipedia_8.json', 'w') as outfile:\n",
    "    json.dump(couple_books_8, outfile, cls=BookJSONEncoder)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40625/40625 [13:34<00:00, 49.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'autheurs en commun:  5952  soit  14.651076923076923 %\n",
      "Nombre d'autheurs reconnus:  10519  soit  1.7673051075268817  auteur dans nos bases de donnée par auteur reconnu dans wikipedia\n",
      "\n",
      "Frequence de reconnaissance des 10 auteurs de wikipédia les plus reconnus:  [('Jean-Pierre Garen', 21), ('Jean-Pierre Muret', 19), ('Jean-Claude Martin', 14), ('Jean-François Merle (écrivain)', 14), ('François Paré', 13), ('Jean-Pierre Balpe', 13), ('Jean-Pierre Martin', 12), ('Pierre Berton (écrivain)', 11), ('Jean-François Parot', 11), ('Jean-François Bladé', 11)]\n",
      "\n",
      "Répartition des livres des auteurs reconnus dans les bases de données: \n",
      "1/8:  {\"Depot_legal\": 12731, \"Babelio\": 2890, \"ILE\": 17054, \"ADP\": 4530}\n",
      "1/6:  {\"ADP\": 793, \"ILE\": 1890, \"Depot_legal\": 2049, \"Babelio\": 390}\n",
      "1/4:  {\"Depot_legal\": 16236, \"Babelio\": 3110, \"ADP\": 7333, \"ILE\": 12731}\n"
     ]
    }
   ],
   "source": [
    "# Couple dont on estime que les deux auteurs sont les mêmes, en fonction du critère (ex: 4 => 1/4)\n",
    "couple_writers_4 = []\n",
    "couple_writers_6 = []\n",
    "couple_writers_8 = []\n",
    "\n",
    "# Compte les livres des auteurs par base de donnée (permet de voir si une base de donnée n'a aucun match)\n",
    "data_base_stats_4 = {}\n",
    "data_base_stats_6 = {}\n",
    "data_base_stats_8 = {}\n",
    "\n",
    "# Compte les auteurs de wikipédia qui ont été trouvé dans nos bases de donnée\n",
    "found_at_least_once = 0\n",
    "# Compte le nombre de couples\n",
    "nb_couple = 0\n",
    "# Compte le nombre de fois qu'un auteur de wikidia été trouvé dans nos bases de donnée\n",
    "author_found_freq = {}\n",
    "\n",
    "with open('../Data/Wikipedia/fr_dumps_wikipedia_writers.json', 'r') as input_file:\n",
    "    wiki_authors = json.load(input_file)\n",
    "\n",
    "for wiki_author, wiki_author_text in tqdm(wiki_authors.items()):\n",
    "    # Compte le nombre de fois que l'auteur wikipédia a été associé a un auteur de nos bases de donnée\n",
    "    wiki_author_found_count = 0\n",
    "    normalized_wiki_author = normalize_author(wiki_author)\n",
    "    for DB_author, DB_author_books in author_ls.items():\n",
    "        dist_auteur = Levenshtein.distance(DB_author, normalized_wiki_author)\n",
    "\n",
    "        if dist_auteur < max(1, min(len(DB_author), len(normalized_wiki_author)) / 4):\n",
    "            new_couple = {\n",
    "                        'author_DB': {'name': DB_author, 'books': DB_author_books},\n",
    "                        'author_wiki': {'name': normalized_wiki_author, 'name_raw': wiki_author, 'text':wiki_author_text}\n",
    "                    }\n",
    "\n",
    "            nb_couple += 1\n",
    "            wiki_author_found_count += 1\n",
    "\n",
    "            if dist_auteur < max(1, min(len(DB_author), len(normalized_wiki_author)) / 8):\n",
    "                    couple_writers_8.append(new_couple)\n",
    "                    for book in DB_author_books:\n",
    "                        try:\n",
    "                            data_base_stats_8[book.data_base] += 1\n",
    "                        except:\n",
    "                            data_base_stats_8[book.data_base] = 1\n",
    "            elif dist_auteur < max(1, min(len(DB_author), len(normalized_wiki_author)) / 6):\n",
    "                couple_writers_6.append(new_couple)\n",
    "                for book in DB_author_books:\n",
    "                    try:\n",
    "                        data_base_stats_6[book.data_base] += 1\n",
    "                    except:\n",
    "                        data_base_stats_6[book.data_base] = 1\n",
    "            else:\n",
    "                couple_writers_4.append(new_couple)\n",
    "                for book in DB_author_books:\n",
    "                    try:\n",
    "                        data_base_stats_4[book.data_base] += 1\n",
    "                    except:\n",
    "                        data_base_stats_4[book.data_base] = 1\n",
    "\n",
    "    if wiki_author_found_count:\n",
    "        found_at_least_once += 1\n",
    "        author_found_freq[wiki_author] = wiki_author_found_count\n",
    "    # if count_freq_i > 10:\n",
    "    #     print(\"compte superieur a 10: \", author_DB)\n",
    "\n",
    "print(\"Nombre d'autheurs en commun: \", found_at_least_once, \" soit \", found_at_least_once * 100 / len(wiki_authors), \"%\")\n",
    "print(\"Nombre d'autheurs reconnus: \", nb_couple, \" soit \", nb_couple/found_at_least_once, ' auteur dans nos bases de donnée par auteur reconnu dans wikipedia')\n",
    "print(\"\\nFrequence de reconnaissance des 10 auteurs de wikipédia les plus reconnus: \", sorted(author_found_freq.items(), key=lambda item: item[1], reverse=True)[:10])\n",
    "\n",
    "\n",
    "print(\"\\nRépartition des livres des auteurs reconnus dans les bases de données: \")\n",
    "print('1/8: ', json.dumps(data_base_stats_8))\n",
    "print('1/6: ', json.dumps(data_base_stats_6))\n",
    "print('1/4: ', json.dumps(data_base_stats_4))\n",
    "\n",
    "with open('../Data/Wikipedia/couple_authors_wikipedia_4.json', 'w') as outfile:\n",
    "    json.dump(couple_writers_4, outfile, cls=BookJSONEncoder)\n",
    "\n",
    "with open('../Data/Wikipedia/couple_authors_wikipedia_6.json', 'w') as outfile:\n",
    "    json.dump(couple_writers_6, outfile, cls=BookJSONEncoder)\n",
    "\n",
    "with open('../Data/Wikipedia/couple_authors_wikipedia_8.json', 'w') as outfile:\n",
    "    json.dump(couple_writers_8, outfile, cls=BookJSONEncoder)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "On peut voir qu'on retrouve assez peu de livres.\n",
    "Comme il est très probable que chaque livre soit relié à la page de son auteurs, on peut se concentrer sur les auteurs\n",
    "et supposer que l'on retrouvera facilement les livres identifiés précédement.\n",
    "\n",
    "### Parsing des informations de la page wikipédia des couples d'auteurs extraits"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2455/2455 [00:12<00:00, 192.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titre des sections les plus fréquentes: \n",
      "Section:  None                            compte:  2455\n",
      "Section:   Liens externes                 compte:  2007\n",
      "Section:   Biographie                     compte:  1746\n",
      "Section:   Notes et références            compte:  1445\n",
      "Section:   Œuvres                         compte:  823\n",
      "Section:   Bibliographie                  compte:  719\n",
      "Section:   Romans                         compte:  526\n",
      "Section:   Œuvre                          compte:  461\n",
      "Section:   Voir aussi                     compte:  422\n",
      "Section:   Références                     compte:  418\n",
      "Section:   Honneurs                       compte:  301\n",
      "Section:   Articles connexes              compte:  297\n",
      "Section:   Publications                   compte:  228\n",
      "Section:   Théâtre                        compte:  214\n",
      "Section:   Annexes                        compte:  212\n"
     ]
    }
   ],
   "source": [
    "with open('../Data/Wikipedia/couple_authors_wikipedia_8.json', 'r') as outfile:\n",
    "    couple_writers = json.load(outfile, cls=BookJSONDecoder)\n",
    "\n",
    "# On compte les intitulés des sections pour la suite\n",
    "title_dict = {}\n",
    "\n",
    "for couple in tqdm(couple_writers):\n",
    "\n",
    "    info_box, page = split_info_box(couple['author_wiki']['text'])\n",
    "\n",
    "    couple['author_wiki']['info_box'] = get_info_from_infobox(info_box)\n",
    "\n",
    "    parsed_page = wtp.parse(format_list(page))\n",
    "    sections_infos = {}\n",
    "    section_titles = set()\n",
    "\n",
    "    for section in parsed_page.sections:\n",
    "        if section.level <= 2:\n",
    "            section_titles.add(section.title)\n",
    "            sections_infos[section.title] = parse_section(section, section_titles, section.level)\n",
    "\n",
    "\n",
    "    couple['author_wiki']['parsed_text'] = sections_infos\n",
    "\n",
    "    # On met a jour le compte des intitulés des sections\n",
    "    for title in section_titles:\n",
    "        try:\n",
    "            title_dict[title] += 1\n",
    "        except KeyError:\n",
    "            title_dict[title] = 1\n",
    "\n",
    "\n",
    "print(\"Titre des sections les plus fréquentes: \")\n",
    "for name, nb in dict(sorted(title_dict.items(), key=lambda item: item[1], reverse=True)[:15]).items():\n",
    "    print('Section: ', str(name) + \" \" * (30 - len(str(name))),  ' compte: ', nb)\n",
    "\n",
    "with open('../Data/Wikipedia/parsed_couple_writers.json', 'w') as outfile:\n",
    "    json.dump(couple_writers, outfile, cls=BookJSONEncoder)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Confirmation des les couples d'auteurs par leurs informations (oeuvres)\n",
    "On s'intéresse aux informations récoltées sur ces auteurs, notament les prix et les oeuvres"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'auteur dont on a récupéré des prix:  519\n",
      "Nombre total de prix:  3658\n",
      "exemple des prix:  ['meilleur roman 2013 le beau mystere ref name nom3', '- golden sheaf award - meilleure emission dramatique - les grands proces - l affaire dion', '60px chevalier de la legion d honneur ref', '- bourse killam', 'grand prix d honneur de l union des ecrivains argentins', 'prix du public au festival d humour de vienne', 'prix l acadie entre les lignes pour le desservant de charnissey', 'prix residence d auteur de la fondation des treilles 2018', 'les aventures d un doudou a travers le monde', 'prix maurice-genevoix pour le chant du grand nord', 'prix de poesie pierrette-micheloud pour ou vont les arbres', 'sceau d argent prix monsieur christie', 'il est decore de l ordre des francophones d amerique en 1985', 'fonds jean-marie-poitras', 'prix juan pablos du merite editorial', 'prix jeunesse des libraires du quebec 2015 categorie 6-11 ans - quebec', 'oprah s book club livre du mois de pour here on earth', '- prix w o mitchell', 'littoral', 'laureate du new jersey author award']\n"
     ]
    }
   ],
   "source": [
    "with open('../Data/Wikipedia/parsed_couple_writers.json', 'r') as input_file:\n",
    "    couples_writers = json.load(input_file, cls=BookJSONDecoder)\n",
    "\n",
    "count_awards = 0\n",
    "nb_awards_for_each_wiki_author = []\n",
    "\n",
    "all_awards_name = set()\n",
    "\n",
    "for couple in couples_writers:\n",
    "    award_titles = set()\n",
    "\n",
    "    for title, section in couple['author_wiki']['parsed_text'].items():\n",
    "        if re.search(r'prix|distinction', title, re.IGNORECASE):\n",
    "            awards = extract_title_from_list(section['list'])\n",
    "            award_titles.update(awards)\n",
    "\n",
    "    if award_titles:\n",
    "        count_awards += 1\n",
    "        all_awards_name.update(award_titles)\n",
    "    nb_awards_for_each_wiki_author.append(len(award_titles))\n",
    "print(\"Nombre d'auteur dont on a récupéré des prix: \", count_awards)\n",
    "print(\"Nombre total de prix: \", sum(nb_awards_for_each_wiki_author))\n",
    "\n",
    "print(\"exemple des prix: \", list(all_awards_name)[:20])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "On veut confirmer nos couples d'auteur à partir des titres des livres qu'ils ont écrits\n",
    "On va comparer les titres uns à uns.\n",
    "- Si un des titre ou plus correspond, alors on confirmera ce couple,\n",
    "- Si aucune titre n'est en commun, on considera le couple comme faux ou incomplet\n",
    "- Si on n'a pas récupéré de titre depuis wikipedia alors on ne peut pas conclure"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2455/2455 [00:20<00:00, 120.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de couple:  2455\n",
      "Nombre de couples confirmés par titres:  1595\n",
      "Nombre de couples infirmés par titres:  642\n",
      "Nombre de couples sans titres de wikipedia:  218\n"
     ]
    }
   ],
   "source": [
    "with open('../Data/Wikipedia/parsed_couple_writers.json', 'r') as couples_file:\n",
    "    couple_writers = json.load(couples_file, cls=BookJSONDecoder)\n",
    "\n",
    "# Listes pour stoker nos couples confirmé, invalidé et ceux dont on ne peut pas conclure car il n'y a pas de titres\n",
    "couples_confirmed = []\n",
    "couples_infirmed = []\n",
    "couples_unsure = []\n",
    "# Compte les cas ci-dessus\n",
    "count_checked_by_titles = 0\n",
    "count_differentiate_by_titles = 0\n",
    "count_no_titles = 0\n",
    "\n",
    "for couple in tqdm(couple_writers):\n",
    "    confirmed_by_title_match = False\n",
    "    confirmed_by_title_match_re = False\n",
    "    book_titles = set()\n",
    "    book_titles_raw = set()\n",
    "\n",
    "    # On recupère tout les titres des livres de la page sous forme de set\n",
    "    for title, section in couple['author_wiki']['parsed_text'].items():\n",
    "        try:\n",
    "            if re.search(r'bibliographie|Œuvres|Œuvre|théâtre|poésie|nouvelles|essais|publications|romans|roman|Littérature|Ouvrages|Filmographie', title, re.IGNORECASE):\n",
    "                book_titles.update(extract_title_from_list(section['list']))\n",
    "                book_titles_raw.update(section['list'])\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    # On vérifie si on peut trouver des titres proches en comparant les titres parsés\n",
    "    # au titres normalisés de nos bases de donnée\n",
    "    book_titles_DB = [book.title for book in couple['author_DB']['books']]\n",
    "    for title in book_titles_DB:\n",
    "        for title_wiki in book_titles:\n",
    "            dist_titles = Levenshtein.distance(title, title_wiki)\n",
    "            if dist_titles < max(1, min(len(title), len(title_wiki)) / 4):\n",
    "                confirmed_by_title_match = True\n",
    "                break\n",
    "    # On recherche par le suite le titre du livre directement dans les titres non nettoyé de la page wikipédia\n",
    "    # via expression régulières\n",
    "    for title in book_titles_DB:\n",
    "        for title_wiki_raw in book_titles_raw:\n",
    "            if re.search(normalize(title), normalize(title_wiki_raw)):\n",
    "                confirmed_by_title_match_re = True\n",
    "                break\n",
    "\n",
    "    # On liste les cas possibles: 0,1,2 tests réussis\n",
    "    if (not confirmed_by_title_match) and (not confirmed_by_title_match_re):\n",
    "        if book_titles and book_titles_DB:\n",
    "            count_differentiate_by_titles += 1\n",
    "            couples_infirmed.append(couple)\n",
    "        else:\n",
    "            count_no_titles += 1\n",
    "            couples_unsure.append(couple)\n",
    "    else:\n",
    "        count_checked_by_titles += 1\n",
    "        couples_confirmed.append(couple)\n",
    "\n",
    "    # On sauvegarde comment avons nous trouvé les correspondances entre les deux listes de titres\n",
    "    couple['confirmed_by_title_match'] = confirmed_by_title_match\n",
    "    couple['confirmed_by_title_match_re'] = confirmed_by_title_match_re\n",
    "\n",
    "print('Nombre de couple: ', len(couple_writers))\n",
    "print('Nombre de couples confirmés par titres: ', count_checked_by_titles)\n",
    "print('Nombre de couples infirmés par titres: ', count_differentiate_by_titles)\n",
    "print('Nombre de couples sans titres de wikipedia: ', count_no_titles)\n",
    "\n",
    "with open('../Data/Wikipedia/couple_authors_wikipedia_confirmed.json', 'w') as outfile:\n",
    "    json.dump(couples_confirmed, outfile, cls=BookJSONEncoder)\n",
    "\n",
    "with open('../Data/Wikipedia/couple_authors_wikipedia_infirmed.json', 'w') as outfile:\n",
    "    json.dump(couples_infirmed, outfile, cls=BookJSONEncoder)\n",
    "\n",
    "with open('../Data/Wikipedia/couple_authors_wikipedia_unsure.json', 'w') as outfile:\n",
    "    json.dump(couples_unsure, outfile, cls=BookJSONEncoder)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "On peut s'interéser à l'intersection des couples généré depuis wikidata"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'auteurs en communs:  776\n"
     ]
    }
   ],
   "source": [
    "with open('../Data/Wikipedia/parsed_couple_writers.json', 'r') as couples_file:\n",
    "    wikipedia_couple_writers = json.load(couples_file)\n",
    "\n",
    "with open('../Data/Wikidata/couple_authors_wikidata.json', 'r') as couples_file:\n",
    "    wikidata_couple_writers = json.load(couples_file)\n",
    "\n",
    "count = 0\n",
    "couples_wikipedia_wikidata = []\n",
    "for wikipedia_couple in wikipedia_couple_writers:\n",
    "    for wikidata_couple in wikidata_couple_writers:\n",
    "        if wikidata_couple['author_DB']['name'] == wikipedia_couple['author_DB']['name']:\n",
    "            count += 1\n",
    "print(\"Nombre d'auteurs en communs: \", count)\n",
    "\n",
    "with open('../Data/couple_authors_wikipedia_wikidata.json', 'w') as outfile:\n",
    "    json.dump(couples_wikipedia_wikidata, outfile)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}